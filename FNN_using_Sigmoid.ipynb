{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FNN_using_Sigmoid.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phuongnguyen99/TwoLayers_Neural_Network/blob/main/FNN_using_Sigmoid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following python implementation of a two layer FNN \n",
        "Step-by-step develop a two layer FNN: \\\n",
        "- Input layer (I): two neurons and one bias neuron, that is $I_1$ and $B_1=1$\n",
        "- Hidden layer (H): two neurons and one bias neuron, that is $H_1$, $H_2$ and $B_2=1$\n",
        "- Output layer (O): one neuron, that is $O_1$\n",
        "\n",
        "We need three weights $w_1$, $w_2$ and $w_3$ from layer H to layer O, no activation function; <br />\n",
        "We need four weights $w_4,\\dots, w_7$ from layer I to layer H, and we use the activation function via sigmoid $\\sigma(x)=\\frac{1}{1+\\exp{(-x)}}$ the derivative is $\\sigma'(x)=\\frac{\\exp(-x)}{\\big(1+\\exp(-x)\\big)^2}$.\n",
        "\n",
        "In summary, we expect the output layer to be\n",
        "$$\n",
        "\\begin{split}\n",
        "O=&H_1\\times w_1 +H_2\\times w_2+ B_2\\times w_3 \\\\\n",
        "=& \\sigma\\big(I_1\\times w_4+B_1\\times w_5\\big)\\times w_1+\\sigma\\big(I_1\\times w_6+B_1\\times w_7\\big)\\times w_2+B_2\\times w_3.\n",
        "\\end{split}\n",
        "$$\n",
        "Let us use the residue sum of squares (RSS) as the cost function to measure the difference between trained O and observed $\\widehat{O}$.\n",
        "$$\n",
        "\\begin{split}\n",
        "\\text{RSS}&=\\frac{1}{2\\times \\text{sample size}}\\|O-\\widehat{O}\\|_2^2=\\frac{1}{2}(O-\\widehat{O})\\cdot (O-\\widehat{O})\\\\\n",
        "&= \\frac{1}{2}\\big(\\sigma\\big(I_1\\times w_4+B_1\\times w_5\\big)\\times w_1+\\sigma\\big(I_1\\times w_6+B_1\\times w_7\\big)\\times w_2+B_2\\times w_3-\\widehat{O}\\big)\\\\\n",
        "&\\qquad \\quad \\cdot \\big(\\sigma\\big(I_1\\times w_4+B_1\\times w_5\\big)\\times w_1+\\sigma\\big(I_1\\times w_6+B_1\\times w_7\\big)\\times w_2+B_2\\times w_3-\\widehat{O}\\big)/\\text{sample size}.\n",
        "\\end{split}\n",
        "$$\n",
        "Now, let's compute the derivative of RSS with respect to each weight. \n",
        "- From layer H to layer O, we define residual (R) to be\n",
        "$$\n",
        "R:=\\sigma\\big(I_1\\times w_4+B_1\\times w_5\\big)\\times w_1+\\sigma\\big(I_1\\times w_6+B_1\\times w_7\\big)\\times w_2+B_2\\times w_3-\\widehat{O},\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial \\text{RSS}}{\\partial w_1}= R\\times \\sigma\\big(I_1\\times w_4+B_1\\times w_5\\big)/\\text{sample size},\n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\frac{\\partial \\text{RSS}}{\\partial w_2}= R\\times\\sigma\\big(I_1\\times w_6+B_1\\times w_7\\big)/\\text{sample size},\n",
        "$$\n",
        "and $\\frac{\\partial \\text{RSS}}{\\partial w_3}=R\\times B_2/\\text{sample size}$.\n",
        "- From layer I to layer H\n",
        "$$\n",
        "\\frac{\\partial \\text{RSS}}{\\partial w_4}= R\\times w_1\\times \\sigma'\\big(I_1\\times w_4+B_1\\times w_5\\big)\\times I_1/\\text{sample size},\n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\frac{\\partial \\text{RSS}}{\\partial w_5}= R\\times w_1 \\times \\sigma'\\big(I_1\\times w_4+B_1\\times w_5\\big)\\times B_1/\\text{sample size},\n",
        "$$\n",
        "and \n",
        "$$\n",
        "\\frac{\\partial \\text{RSS}}{\\partial w_6}= R\\times w_2\\times \\sigma'\\big(I_1\\times w_6+B_1\\times w_7\\big)\\times I_1/\\text{sample size},\n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\frac{\\partial \\text{RSS}}{\\partial w_7}= R\\times w_2 \\times \\sigma'\\big(I_1\\times w_6+B_1\\times w_7\\big)\\times B_1/\\text{sample size}.\n",
        "$$\n"
      ],
      "metadata": {
        "id": "CwgzryGm_VAo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-JFVyDea8gX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_function(x):\n",
        "  y = 1/(1+ np.exp(-x))\n",
        "  return y "
      ],
      "metadata": {
        "id": "ZeoRrj8jbB6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implement derivative of sigmoid function \n",
        "def sigmoid_derivative(x):\n",
        "  dy = np.exp(-x)/(1+np.exp(-x))**2\n",
        "  return dy"
      ],
      "metadata": {
        "id": "bAYxAgnMbHpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implement FNN\n",
        "# (x, y) input data\n",
        "# lr: learning rate\n",
        "def two_layer(x,y,lr):\n",
        "# This function use Gradient descent with fixed learning rate to train the two layer\n",
        "# feedforward neural network: \n",
        "  max_ite = 200000\n",
        "  tolerance = 0.001\n",
        "# Initialize the weights W by all zeros entries\n",
        "  W = np.zeros(7)\n",
        "  # W0 = W\n",
        "  B1 = np.ones_like(x) # bias node at the input layer \n",
        "  B2 = np.ones_like(x) # bias node at the hidden layer\n",
        "  H1 = np.zeros_like(x)\n",
        "  H2 = np.zeros_like(x)\n",
        "\n",
        "  ite = 0 \n",
        "  loss = 1\n",
        "  out, H1, H1_tidle, H2, H2_tidle, R, loss = forward_computation(x,y,W)\n",
        "\n",
        "# Apply Gradient descent with fixed learning rate\n",
        "  while ite <= max_ite and loss>tolerance:\n",
        "\n",
        "    # Do the backward propogation to update the weights W\n",
        "    grad_W = backward_computation(x,y,W,B1,B2,H1, H1_tidle, H2, H2_tidle, R)\n",
        "    W = W- grad_W*lr\n",
        "  \n",
        "\n",
        "    # Do the forward propogation to update the FNN and lost function \n",
        "    out, H1, H1_tidle, H2, H2_tidle, R, loss = forward_computation(x,y,W)\n",
        "  \n",
        "    # # Every 100 iteration steps, print ite and loss values\n",
        "    # if ite %100 == 0:\n",
        "    #    print(ite,':and' ,loss)\n",
        "    # ite += 1\n",
        "  \n",
        "\n",
        "  return out, loss, W\n",
        "\n"
      ],
      "metadata": {
        "id": "xIZdcg--bTah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def  forward_computation(x,y,W):\n",
        "   # This function update the values of FNN\n",
        "      # W = np.ones(7)\n",
        "      b1 = np.ones_like(x)\n",
        "      b2 = np.ones_like(x)\n",
        "      H1 = x*W[3]+ b1*W[4]\n",
        "      H1_tidle = sigmoid_function(H1)\n",
        "      H2 = x*W[5]+ b1*W[6]\n",
        "      H2_tidle = sigmoid_function(H2)\n",
        "      out = H1_tidle*W[0] + H2_tidle*W[1] +b2*W[2]\n",
        "      R = (out - y)\n",
        "      # The loss should be renormalized by the total sample size\n",
        "      loss = 0.5*np.dot(R,R)/len(y)\n",
        "      return out, H1, H1_tidle, H2, H2_tidle, R, loss\n"
      ],
      "metadata": {
        "id": "H7mrYkAibTif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_computation(x,y,W,B1,B2,H1, H1_tidle, H2, H2_tidle, R):\n",
        "  # This function compute the gradient of the FNN\n",
        "\n",
        "  grad_W = np.zeros_like(W)\n",
        "  grad_W[0] = np.dot(R,H1_tidle)\n",
        "  grad_W[1] = np.dot(R,H2_tidle)\n",
        "  grad_W[2] = np.dot(R,B2)\n",
        "  grad_W[3] = W[0]*np.dot(R, np.multiply(sigmoid_derivative(H1),x))\n",
        "  grad_W[4] = W[0]*np.dot(R, np.multiply(sigmoid_derivative(H1),B1))\n",
        "  grad_W[5] = W[1]*np.dot(R, np.multiply(sigmoid_derivative(H2),x))\n",
        "  grad_W[6] = W[1]*np.dot(R, np.multiply(sigmoid_derivative(H2),B2))\n",
        "  grad_W= grad_W/len(y) # renormalize the gradient with total sample size\n",
        "    \n",
        "  return grad_W"
      ],
      "metadata": {
        "id": "6hr-437ib-v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the FNN\n",
        "#x = np.array([1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,6.5])\n",
        "x = np.linspace(-1,1,200)\n",
        "x2 = x.T\n",
        "y = (x2-0.2)**2 + np.random.randn(x2.size)*0\n",
        "W = np.zeros(7)\n",
        "lr = 0.05\n",
        "out, loss, W = two_layer(x2,y,lr)\n",
        "\n",
        "plt.plot(x2,y, '--')\n",
        "plt.plot(x,out, 'x')\n"
      ],
      "metadata": {
        "id": "fwL3ymIncgxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0, 1,1001).T\n",
        "y = np.sin(x)+x+(x)**3+0.05*np.random.randn(x.size)\n",
        "out, loss,W = two_layer(x,y,lr=1e-1)\n",
        "plt.plot(x,y,'--')\n",
        "plt.plot(x,out, 'x')\n"
      ],
      "metadata": {
        "id": "jkyMN2V5doKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0, 1,1001).T\n",
        "y = x+(x)**3+0.05*np.random.randn(x.size)\n",
        "out, loss,W = two_layer(x,y,lr=5e-2)\n",
        "plt.plot(x,y,'--')\n",
        "plt.plot(x,out, 'x')\n"
      ],
      "metadata": {
        "id": "hSW9oIDi8HTR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}